{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mvqaj3-OgbwK",
        "e-LuYR1S6V0j",
        "LguKwDmmCGKO",
        "8OZqWURtCKyM",
        "rrzg2DffCe82",
        "nRtNjeciM6TW",
        "IbVGBViv6mzs",
        "26e3_mS0NLe1",
        "ttcmKtFcdnPh",
        "vKXHiIvZBrP3",
        "zdbM83EmEAGc",
        "eRX7GKu3k2pg"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvqaj3-OgbwK"
      },
      "source": [
        "## **server*NMT*&reg; with OpenNMT-py 2.0**\n",
        "*Streamlining workflows for Neural Machine Translation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75QmqCqKp_47"
      },
      "source": [
        "![DCU](https://github.com/seamusl/nmt/blob/main/logos/dcu.png?raw=true) ![MTU](https://github.com/seamusl/nmt/blob/main/logos/mtu.png?raw=true)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBhik0Tg_YhD"
      },
      "source": [
        "# **Readme** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDupclRAt8BI"
      },
      "source": [
        "*  Specify directory on your gdrive where models and results are to be stored.\n",
        "*  Upload a zip file containing the following files:\n",
        "   * src-val.txt, tgt-val.txt, src-test.txt, tgt-test.txt\n",
        "   * src-train.txt, tgt-train.txt\n",
        "   * vanilla.yaml, transformer.yaml (defaults downloaded if not specified)\n",
        "*   The Experiment log contains the following evaluations:\n",
        "    *   Bleu corpus level (mixed case), Bleu corpus level (lower case)\n",
        "    *   Chrf 1, Chrf 3, Meteor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-LuYR1S6V0j"
      },
      "source": [
        "MIT License\n",
        "\n",
        "##### © 2023 Adapt Centre, DCU / MTU, Ireland.\n",
        "##### Author: Séamus Lankford  \n",
        "##### seamus.lankford[at]adaptcentre.ie\n",
        "\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEfhFWT9myqd"
      },
      "source": [
        "![MIT-license](https://github.com/seamusl/nmt/blob/main/logos/mit_license.png?raw=true) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LguKwDmmCGKO"
      },
      "source": [
        "## **Citation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OZqWURtCKyM"
      },
      "source": [
        "### **Using bibtex**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0BAOZyIC3rj"
      },
      "source": [
        "```\n",
        "@misc{lankford_way_alfi_2023,\n",
        " title={serverNMT with OpenNMT 2.0}\n",
        " url={https://github.com/adaptNMT},\n",
        " publisher={Adapt Centre, Dublin City University}, \n",
        " author={Lankford, Seamus and Way, Andy and Alfi, Haithem}, year={2021}, month={Mar}\n",
        " } \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrzg2DffCe82"
      },
      "source": [
        "### Using text citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7CtoiosCQoM"
      },
      "source": [
        "Lankford, S., Way, A., &amp; Alfi, H. (2023, January 22). serverNMT with OpenNMT 2.0 (Version 1.0) [Computer software]. Retrieved from https://github.com/adaptNMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HkzuAsZsr-Z"
      },
      "source": [
        "\n",
        "# **Initialize**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iip8QmGgOZv"
      },
      "source": [
        "Some, or all, of the following may need to be installed outside of colab depending on what machine the server application is installed.\n",
        "```\n",
        "sudo apt-get install python3.7\n",
        "# following is required to be installed locally for sentencepiece to work\n",
        "sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev\n",
        "# git must be installed locally in order to pull down repos\n",
        "sudo apt install git\n",
        "sudo apt install gnome-online-accounts\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRtNjeciM6TW"
      },
      "source": [
        "## **Jupyter notebook server**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbVGBViv6mzs"
      },
      "source": [
        "### Setup Jupyter notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_pUkbG65x3"
      },
      "source": [
        "In order to allow Colaboratory to connect to your locally running Jupyter server, you'll need to perform the following steps.\n",
        "\n",
        "Step 1: Install Jupyter. Install Jupyter on your local machine.\n",
        "\n",
        "Step 2: Install and enable the jupyter_http_over_ws jupyter extension (one-time). \n",
        "\n",
        "The jupyter_http_over_ws extension is authored by the Colaboratory team and available on GitHub.\n",
        "\n",
        "\n",
        "```\n",
        "pip3 install jupyterlab\n",
        "pip3 install jupyter_http_over_ws\n",
        "sudo apt  install jupyter-core\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e3_mS0NLe1"
      },
      "source": [
        "### Start server and authenticate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzGyzbCE7Tpc"
      },
      "source": [
        "New notebook servers are started normally, though you will need to set a flag to explicitly trust WebSocket connections from the Colaboratory frontend.\n",
        "\n",
        "```\n",
        "jupyter notebook \\\n",
        "  --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "  --port=8888 \\\n",
        "  --NotebookApp.port_retries=0\n",
        "```\n",
        "\n",
        "Once the server has started, it will print a message with the initial backend URL used for authentication. Make a copy of this URL as you'll need to provide this in the next step.\n",
        "\n",
        "Step 4: Connect to the local runtime\n",
        "In Colaboratory, click the \"Connect\" button and select \"Connect to local runtime...\". Enter the URL from the previous step in the dialog that appears and click the \"Connect\" button. After this, you should now be connected to your local runtime.\n",
        "\n",
        "Browser-specific settings\n",
        "Note: If you're using Mozilla Firefox, you'll need to set thenetwork.websocket.allowInsecureFromHTTPS preference within the Firefox config editor. Colaboratory makes a connection to your local kernel using a WebSocket. By default, Firefox disallows connections from HTTPS domains using standard WebSockets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9rXMFjTNmA-"
      },
      "source": [
        "If Jupyter fails to start on system, upgrading ipykernel may fixes it:\n",
        "\n",
        "```\n",
        "pip3 install --upgrade ipykernel\n",
        "```\n",
        "\n",
        "See issues:\n",
        "ipython/ipython#11258 and ipython/ipykernel#335\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjaDBeuSw9T_"
      },
      "source": [
        "\n",
        "# **Setup Environment**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM0C7al8AUdu"
      },
      "source": [
        "#@markdown ### Directory for results and models:\n",
        "results_dir = \"test_server\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Cell debug:\n",
        "debug_on = True #@param {type:\"boolean\"}\n",
        "\n",
        "noise = '&> /dev/null'\n",
        "if debug_on:\n",
        "  noise = ''\n",
        "\n",
        "HOME = '~'\n",
        "%cd $HOME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7wprmMzkCKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1bd1f4-ec2b-4e4a-c4f8-f8846ac52394"
      },
      "source": [
        "import os\n",
        "exists = os.path.isdir('$results_dir')\n",
        "if not exists:\n",
        "# recursively create the results directory with its parents\n",
        "  %mkdir -p $results_dir\n",
        "  %cd $HOME/$results_dir\n",
        "  %mkdir data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/seamus/test_server\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gn4Xt2G-3vk"
      },
      "source": [
        "%cd $HOME\n",
        "# install OpenNMT 2.0 in google home dir\n",
        "''' <pip install opennmt> installs old version of OpenNMT so instead \n",
        "must download OpenNMT source and build'''\n",
        "\n",
        "import os\n",
        "exists = os.path.isdir('OpenNMT-py')\n",
        "if not exists:\n",
        "  !git clone https://github.com/OpenNMT/OpenNMT-py.git $noise\n",
        "\n",
        "%cd OpenNMT-py/\n",
        "%pwd\n",
        "!pip3 install -e . $noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NdOMxvBJYPZ",
        "cellView": "form"
      },
      "source": [
        "#@title Vocab size, submodel type and input dataset\n",
        "\n",
        "# install sentencepiece in google home dir\n",
        "%cd $HOME \n",
        "\n",
        "use_sub_model = False #@param {type:\"boolean\"}\n",
        "v_size = 32000 #@param {type:\"number\"}\n",
        "m_type = 'bpe' #@param [\"unigram\", \"bpe\"]\n",
        "\n",
        "''' <pip install sentencepiece> doesn't install properly so\n",
        "must download source and build \n",
        "More info: https://github.com/google/sentencepiece ''' \n",
        "\n",
        "if use_sub_model == True:\n",
        "  !git clone https://github.com/google/sentencepiece.git\n",
        "  %cd sentencepiece\n",
        "  %mkdir build\n",
        "  %cd build\n",
        "  ! cmake .. $noise\n",
        "  ! make -j $(nproc) $noise\n",
        "  ! make install $noise\n",
        "  ! ldconfig -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttcmKtFcdnPh"
      },
      "source": [
        "### **Setup graphics and packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebf690c-7b95-41f0-d7c6-f342c4a662c7"
      },
      "source": [
        "# Display GPU details provided by Google \n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu \\\n",
        "    to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  print(gpu_info, file=open(\"experiment_log.txt\", \"a\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar 28 11:49:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  GeForce GTX 1080    Off  | 00000000:01:00.0 Off |                  N/A |\n",
            "| 24%   47C    P5    13W / 180W |    497MiB /  8119MiB |     32%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A       969      G   /usr/lib/xorg/Xorg                 45MiB |\n",
            "|    0   N/A  N/A      1571      G   /usr/lib/xorg/Xorg                265MiB |\n",
            "|    0   N/A  N/A      1771      G   /usr/bin/gnome-shell               35MiB |\n",
            "|    0   N/A  N/A    357545      G   /usr/lib/firefox/firefox            1MiB |\n",
            "|    0   N/A  N/A    357578      G   /usr/lib/firefox/firefox            1MiB |\n",
            "|    0   N/A  N/A    357898      G   ...AAAAAAAAA= --shared-files      135MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXN_SwEJjr-B"
      },
      "source": [
        "# tee command reads standard input and writes it to both standard output and\n",
        "# one or more files. Useful for logging corpora details and evaluation results\n",
        "!pip3 install tee $noise\n",
        "\n",
        "# execution time of each cell tracked with following package. \n",
        "# useful for tracking model training times\n",
        "!pip3 install ipython-autotime $noise\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZVb5sEOiObx"
      },
      "source": [
        "# **Anvil Server**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxTqIMjRNA-Q"
      },
      "source": [
        "## **Setup local Anvil server**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkxOX1jNmhmt"
      },
      "source": [
        "### Enter the Uplink key from your Anvil web app.\n",
        "For information on how to get your apps Uplink key, see [Step 4 - Enable the Uplink](https://anvil.works/learn/tutorials/google-colab-to-web-app#step-4-enable-the-uplink)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYIAacDZmQ__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e97de70-75e1-4d69-92da-c29a8cfaaa88"
      },
      "source": [
        "from getpass import getpass\n",
        "uplink_key = getpass('Enter your Uplink key: ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your Uplink key: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6AQWVo1iOEz"
      },
      "source": [
        "!pip3 install anvil-uplink\n",
        "import anvil.server\n",
        "anvil.server.connect(uplink_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Ho2NlGjlRs"
      },
      "source": [
        "# **API functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adJ0_fzKO-cN"
      },
      "source": [
        "### **Split input datasets into train, test and val sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMFC0RRaucRt"
      },
      "source": [
        "@anvil.server.callable\n",
        "\n",
        "def split_train_val_test(src, tgt):\n",
        "\n",
        "  %cd $user_dir/data/\n",
        "  train_percent = 92.5\n",
        "  valid_percent = 5 \n",
        "  test_percent = 2.5 \n",
        "\n",
        "  import random\n",
        "  import math\n",
        "\n",
        "  S_DATASET = src\n",
        "  T_DATASET = tgt\n",
        "\n",
        "  s_data = [l for l in open(S_DATASET, 'r')]\n",
        "  t_data = [l for l in open(T_DATASET, 'r')]\n",
        "\n",
        "  s_train_file = open('src-train.txt', 'w')\n",
        "  s_valid_file = open('src-val.txt', 'w')\n",
        "  s_tests_file = open('src-test.txt', 'w')\n",
        "\n",
        "  t_train_file = open('tgt-train.txt', 'w')\n",
        "  t_valid_file = open('tgt-val.txt', 'w')\n",
        "  t_tests_file = open('tgt-test.txt', 'w')\n",
        "\n",
        "  # s_data and t_data must be same lenght for parallel set\n",
        "  num_of_data = len(s_data)\n",
        "\n",
        "  num_train = int((train_percent/100.0)*num_of_data)\n",
        "  num_valid = int((valid_percent/100.0)*num_of_data)\n",
        "  num_tests = int((test_percent/100.0)*num_of_data)\n",
        "\n",
        "  data_splits = [num_train, num_valid, num_tests]\n",
        "\n",
        "  s_split_data = [[],[],[]]\n",
        "  t_split_data = [[],[],[]]\n",
        "\n",
        "  rand_data_ind = 0\n",
        "\n",
        "  for split_ind, fraction in enumerate(data_splits):\n",
        "    for i in range(fraction):\n",
        "      rand_data_ind = random.randint(0, len(s_data)-1)\n",
        "      s_split_data[split_ind].append(s_data[rand_data_ind])\n",
        "      s_data.pop(rand_data_ind)\n",
        "      t_split_data[split_ind].append(t_data[rand_data_ind])\n",
        "      t_data.pop(rand_data_ind)\n",
        "      \n",
        "  for l in s_split_data[0]:\n",
        "    s_train_file.write(l)\n",
        "      \n",
        "  for l in s_split_data[1]:\n",
        "    s_valid_file.write(l)\n",
        "        \n",
        "  for l in s_split_data[2]:\n",
        "    s_tests_file.write(l)\n",
        "        \n",
        "  s_train_file.close()\n",
        "  s_valid_file.close()\n",
        "  s_tests_file.close()\n",
        "\n",
        "  for l in t_split_data[0]:\n",
        "    t_train_file.write(l)\n",
        "        \n",
        "  for l in t_split_data[1]:\n",
        "      t_valid_file.write(l)\n",
        "        \n",
        "  for l in t_split_data[2]:\n",
        "      t_tests_file.write(l)\n",
        "        \n",
        "  t_train_file.close()\n",
        "  t_valid_file.close()\n",
        "  t_tests_file.close()\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsg4JhzJC9Dc"
      },
      "source": [
        "### **Common API functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGDyXHKC9Dd"
      },
      "source": [
        "'''\n",
        "With pytorch, need to use tensorboardX instead of tensorboard for visualisation\n",
        "https://github.com/lanpa/tensorboardX \n",
        "https://tensorboardx.readthedocs.io/en/latest/tutorial.html \n",
        "https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/utils/statistics.py \n",
        "# !pip install -U tensorboard-plugin-profile\n",
        "'''\n",
        "@anvil.server.callable\n",
        "def visualize():\n",
        "  %cd $HOME/$results_dir\n",
        "  !pip install tensorboardX &> /dev/null\n",
        "\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6uccO-jGiOY"
      },
      "source": [
        "# following required once off to make /tmp writeable:\n",
        "# sudo chmod a=rwx,u+t /tmp\n",
        "# this function creates temp file for omnt_translate\n",
        "\n",
        "@anvil.server.callable\n",
        "def make_file():\n",
        "  %cd /tmp\n",
        "  tfile=!$(mktemp)\n",
        "  !ls -t | head -n1\n",
        "  recent = !ls -t | head -n1\n",
        "  temp_file = recent[0]\n",
        "  return temp_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKXHiIvZBrP3"
      },
      "source": [
        "## serverNMT&reg; Translate##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31fYLSHSI2QR"
      },
      "source": [
        "# function takes incoming source string and returns target translation\n",
        "@anvil.server.callable\n",
        "def tran_api_sent(str):\n",
        "  print(str)\n",
        "  src_temp = make_file()\n",
        "  # store source string in a temporary file\n",
        "  !echo $str >> $src_temp \n",
        "  translation_model = \"model_step_100000.pt\"\n",
        "  !echo -e \"-Translating sentence ....\" $translation_model | tee -a server_log.txt\n",
        "  \n",
        "  %cd ~/production/\n",
        "  full_prediction = !onmt_translate --model $translation_model \\\n",
        "          --src /tmp/$src_temp \\\n",
        "          --output pred \\\n",
        "          -replace_unk -verbose\n",
        "\n",
        "  !echo $full_prediction\n",
        "  prediction = !echo $full_prediction | sed 's/.*PRED 1\\(.*\\)PRED SCORE.*/\\1/'\n",
        "  return prediction "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2UdEhpPdiqj",
        "outputId": "688ca8d4-3ecb-4145-f654-90f1a6683744"
      },
      "source": [
        "import anvil.media\n",
        "\n",
        "@anvil.server.callable\n",
        "def translate_api(file):\n",
        "  %cd ~/production\n",
        "  with anvil.media.TempFile(file) as src_tmp:\n",
        "    print(\"printing the file ..\")\n",
        "    !echo $src_tmp\n",
        "    translation_model = \"model_step_100000.pt\"\n",
        "    !echo -e \"-Translating file ....\" $translation_model | tee -a server_log.txt\n",
        "    prediction = !onmt_translate --model $translation_model \\\n",
        "          --src $src_tmp \\\n",
        "          --output pred.txt \\\n",
        "          -replace_unk -verbose\n",
        "    with open('pred.txt', 'r') as f:\n",
        "      contents = f.read()      \n",
        "  return contents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.8 ms (started: 2021-03-23 19:59:45 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMXv0vBiB5bi"
      },
      "source": [
        "## serverNMT&reg; Build##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybWWayggvTqB"
      },
      "source": [
        "\n",
        "### **Auto NMT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z83rLb9-FoDq"
      },
      "source": [
        "import anvil.media\n",
        "\n",
        "# Write the byte contents of the media object to 'tmp/my-file.txt'\n",
        "# (we opened the file in binary mode, so f.write() accepts bytes)\n",
        "@anvil.server.callable\n",
        "def store_src(file):\n",
        "  user_dir = make_dir()\n",
        "  %cd $user_dir\n",
        "  with open('src.txt', 'wb+') as f:\n",
        "    f.write(file.get_bytes())\n",
        "  return\n",
        "\n",
        "@anvil.server.callable\n",
        "def store_tgt(file):\n",
        "  user_dir = make_dir()\n",
        "  %cd $user_dir\n",
        "  with open('tgt.txt', 'wb+') as f:\n",
        "    f.write(file.get_bytes())\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn5ZEIqtIbtS"
      },
      "source": [
        "# following required once off to make /tmp writeable:\n",
        "# sudo chmod a=rwx,u+t /tmp\n",
        "\n",
        "# this function creates temp file for omnt_translate\n",
        "@anvil.server.callable\n",
        "def make_dir():\n",
        "  %cd /tmp\n",
        "  tfile=!\"$(mktemp -d)\"\n",
        "  t_dir=!ls -t | head -n1\n",
        "  tdir=t_dir[0]\n",
        "  %cd $tdir\n",
        "  %mkdir data\n",
        "  print(\"in make_dir\")\n",
        "  return tdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XbNVHJGbb4x"
      },
      "source": [
        "@anvil.server.callable\n",
        "def button2():\n",
        "  print(\"in button 2\")\n",
        "  button2_return = \"Getting there\"\n",
        "  return button2_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQdo79uEJLW"
      },
      "source": [
        "import anvil.media\n",
        "\n",
        "@anvil.server.callable\n",
        "def build(mode, model_type):\n",
        "  visualize()\n",
        "  user_dir = make_dir()\n",
        "  %cd /tmp/$user_dir\n",
        "  !cp /home/seamus/production/defaults/* ./data\n",
        "  if mode == 'vanilla':\n",
        "    config = set_vanilla(model_type)\n",
        "  else:\n",
        "    config = set_transformer(model_type)\n",
        "\n",
        "  import time\n",
        "  start_train = time.time()\n",
        "\n",
        "  print(\"calling split now ..\")\n",
        "#  %cd data\n",
        "#  %pwd\n",
        "# split_train_val_test()\n",
        "\n",
        "#  if use_sub_model:\n",
        "#    !cat src-train.txt tgt-train.txt> train.txt\n",
        "#    !wc tgt-train.txt | tee -a experiment_log.txt\n",
        "#    !spm_train --input='train.txt' --model_prefix=spm \\\n",
        "#      --vocab_size=$v_size --character_coverage=1.0 --model_type='bpe'\n",
        "\n",
        "  !onmt_build_vocab -config $config -n_sample=-1\n",
        "  !onmt_train -config $config\n",
        "  !cat $config | tee -a data/experiment_log.txt\n",
        "\n",
        "  end_train = time.time()\n",
        "  train_time = int(end_train - start_train)\n",
        "  print(train_time)\n",
        "  print(\"+ Model Training Time + \\n\" + str(train_time), \\\n",
        "        file=open(\"data/experiment_log.txt\", \"a\"))\n",
        "\n",
        "  build_return = 1\n",
        "  return build_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznuD7-gwqfM"
      },
      "source": [
        "# download any missing files from github.com/seamusl/nmt repo\n",
        "@anvil.server.callable\n",
        "def set_vanilla(vanilla_type):\n",
        "  print(\"in set vanilla\")\n",
        "  if vanilla_type == 'Base':\n",
        "    config = \"data/vanilla.yaml\"\n",
        "  elif vanilla_type == 'Adam':\n",
        "    config = \"data/vanilla_adam.yaml\"\n",
        "  elif vanilla_type == 'BPE':\n",
        "    config = \"data/vanilla_bpe.yaml\"\n",
        "  else:\n",
        "    config = \"data/config.yaml\" \n",
        "  return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2rrY6mOJXNY"
      },
      "source": [
        "# download any missing files from github.com/seamusl/nmt repo\n",
        "@anvil.server.callable\n",
        "def set_transformer(transformer_type):\n",
        "  if use_transformer:\n",
        "    if transformer_type == 'Base':\n",
        "      config = \"data/transformer.yaml\"\n",
        "    elif transformer_type == 'BPE':\n",
        "      config = \"data/transformer_bpe.yaml\"\n",
        "    else:\n",
        "      config = \"data/config.yaml\" \n",
        "  return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjAN5bLiRkx"
      },
      "source": [
        "## **Run server**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BMAM0IgiR9a"
      },
      "source": [
        "anvil.server.wait_forever() \n",
        "# ! >> server_log.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwZWgt6cQPFg"
      },
      "source": [
        "# **Acknowledgments**\n",
        "\n",
        "\n",
        "This work was supported by the ADAPT Centre, which is funded under the SFI Research Centres\n",
        "Programme (Grant 13/RC/2016) and is co-funded by the European Regional Development Fund.\n",
        "\n",
        "\n",
        "\n",
        "> ![Adapt](https://github.com/seamusl/nmt/blob/main/logos/adapt.png?raw=true)\n",
        "\n",
        "\n",
        ">![SFI](https://github.com/seamusl/nmt/blob/main/logos/sfi.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdbM83EmEAGc"
      },
      "source": [
        "# **References**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUSSaKiBd3O"
      },
      "source": [
        "Nvidia driver can be chosen by displaying drivers.\n",
        "\n",
        "Select recommended driver and install.\n",
        "\n",
        "https://www.howtogeek.com/451262/how-to-use-rclone-to-back-up-to-google-drive-on-linux/ \n",
        "\n",
        "https://rclone.org/drive/\n",
        "\n",
        "\n",
        "https://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/#:~:text=Opening%20Crontab,if%20you're%20using%20Ubuntu.&text=Use%20the%20crontab%20%2De%20command,with%20your%20user%20account's%20permissions.\n",
        "\n",
        "```\n",
        "# install pip3\n",
        "sudo apt-get install python3-pip\n",
        "\n",
        "# choose optimal Nvidia driver\n",
        "ubuntu-drivers devices\n",
        "sudo apt install nvidia-driver-460\n",
        "\n",
        "# setup cron job for rclone to check production server every 15 minutes \n",
        "sudo crontab -e\n",
        "# enter the following job\n",
        "0,14,29,44 * * * * /usr/bin/rclone copy --update --verbose --transfers 30 --checkers 8 --contimeout 60s --timeout 300s --retries 3 \\ \n",
        "--low-level-retries 10 --stats 1s \"mygoogledrive:/\" \"/home/seamus/production\"\n",
        "\n",
        "# to view contents of cloned drive (only production server cloned using root_id in the rclone config file)\n",
        "rclone ls mygoogledrive:/\n",
        "\n",
        "# to clone googledrive to local directory \n",
        "/usr/bin/rclone copy --update --verbose --transfers 30 --checkers 8 --contimeout 60s --timeout 300s --retries 3 \\ \n",
        "--low-level-retries 10 --stats 1s \"mygoogledrive:/\" \"/home/seamus/production\"\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRX7GKu3k2pg"
      },
      "source": [
        "# **Translate**\n",
        "\n",
        "Specify the model to be used for translation.\n",
        "\n",
        "An ensemble of models can be used for translation by specifying multiple models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvURB55H3AMK"
      },
      "source": [
        "%cd /home/seamus/OpenNMT-py/\n",
        "\n",
        "!ls\n",
        "translation_model1 = \"model_step_100000.pt\" \n",
        "\n",
        "!echo -e \"-Translating using ....\" $translation_model1\n",
        "\n",
        "!onmt_translate --model $translation_model1 --src src-test.txt \\\n",
        "          --output pred.txt -replace_unk -verbose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDforv5MKKrd"
      },
      "source": [
        "import anvil.media\n",
        "\n",
        "@anvil.server.callable\n",
        "def setup_build(src, tgt):\n",
        "  ## create a build directory using randomly generated name\n",
        "  user_dir = mkdir()\n",
        "  %cd $user_dir\n",
        "  %mkdir data\n",
        "  config = set_config()\n",
        "  with anvil.media.TempFile(src) as src_tmp:\n",
        "    with open('src.txt', 'w') as f:\n",
        "      contents = f.write()\n",
        "    with open('tgt.txt', 'w') as f:\n",
        "      contents = f.write()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xryFn-plSGJZ"
      },
      "source": [
        "@anvil.server.callable\n",
        "def test_api():\n",
        "  msg = \"Translating .. Yipee\"\n",
        "  print(\"Hello from the uplink\")\n",
        "  return msg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhcBCsl_Fsjs"
      },
      "source": [
        "@anvil.server.callable\n",
        "def translate_api_sent():\n",
        "  %cd ~/production\n",
        "  translation_model = \"model_step_100000.pt\"\n",
        "  !echo -e \"-Translating sentence using ....\" translation_model | tee -a server_log.txt\n",
        "  %cd ~/production/\n",
        "  print(\"Not using subword\")\n",
        "  prediction = !onmt_translate --model $translation_model \\\n",
        "          --src \"Hello\" \\\n",
        "          --output pred.txt \\\n",
        "          -replace_unk -verbose\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SdXvFiJ3TcY"
      },
      "source": [
        "!whoami\n",
        "%cd /tmp\n",
        "!echo \"hello hello\" >> tmp.4tqAVHvxnr\n",
        "#make_file(\"hello\")\n",
        "#!cat $tfile  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNh0zEQEkPsu"
      },
      "source": [
        "@anvil.server.callable\n",
        "def say_hello(name):\n",
        "  print(\"Hello from the uplink, %s!\" % name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V729uvo41yX5"
      },
      "source": [
        "def make_file(str):\n",
        "  tfile=!$(mktemp)\n",
        "  !ls -t | head -n1\n",
        "  temp_file = !ls -t | head -n1\n",
        "  !echo $str >> $temp_file\n",
        "  return\n",
        "make_file(\"MTU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l43ivEtGaFw",
        "outputId": "e2066a93-1260-41cb-9efe-7d686afb2492"
      },
      "source": [
        "@anvil.server.callable\n",
        "def file_processor(file):\n",
        "  \n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.22 ms (started: 2021-03-23 19:27:02 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PLCKNSKqkmV"
      },
      "source": [
        "@anvil.server.callable\n",
        "def tran_api_sent(src_text):\n",
        "  src_temp = make_file()\n",
        "  !echo \"just called make_file\"\n",
        "  print(src_text)  \n",
        "  # >> /tmp/$src_temp \n",
        "  translation_model = \"model_step_100000.pt\"\n",
        "  #!echo -e \"-Translating sentence using ....\" $translation_model | tee -a server_log.txt\n",
        "  %cd ~/production/\n",
        "\n",
        "  prediction = !onmt_translate --model $translation_model \\\n",
        "          --src /tmp/$src_temp \\\n",
        "          --output pred \\\n",
        "          -replace_unk -verbose\n",
        "  \n",
        "  return prediction "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysFusiHG8I--"
      },
      "source": [
        "@anvil.server.callable\n",
        "def auto_build(user_dir):\n",
        "  import time\n",
        "  start_train = time.time()\n",
        "  %cd data\n",
        "  %pwd\n",
        "  print(\"calling split now ..\")\n",
        "  #split_train_val_test()\n",
        "\n",
        "#  if use_sub_model:\n",
        "#    !cat src-train.txt tgt-train.txt> train.txt\n",
        "#    !wc tgt-train.txt | tee -a experiment_log.txt\n",
        "#    !spm_train --input='train.txt' --model_prefix=spm \\\n",
        "#      --vocab_size=$v_size --character_coverage=1.0 --model_type='bpe'\n",
        "\n",
        "  %cd $user_dir\n",
        "  !onmt_build_vocab -config $config -n_sample=-1\n",
        "  !onmt_train -config $config\n",
        "  !cat $config | tee -a data/experiment_log.txt\n",
        "\n",
        "  end_train = time.time()\n",
        "  train_time = int(end_train - start_train)\n",
        "  print(train_time)\n",
        "  print(\"+ Model Training Time + \\n\" + str(train_time), \\\n",
        "        file=open(\"data/experiment_log.txt\", \"a\"))\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}